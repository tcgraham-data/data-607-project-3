---
title: "Project 3 (Group)"
author: "Team A1"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# You will need to determine what tool(s) you’ll use as a group to effectively collaborate, share code and any project documentation (such as motivation, approach, findings). 

# You will have to determine what data to collect, where the data can be found, and how to load it. 

# The data that you decide to collect should reside in a relational database, in a set of normalized tables. 

# You should perform any needed tidying, transformations, and exploratory data analysis in R. 

# Your deliverable should include all code, results, and documentation of your motivation, approach, and findings. 

# As a group, you should appoint (at least) three people to lead parts of the presentation. 

# While you are strongly encouraged (and will hopefully find it fun) to try out statistics and data models, your grade will not be affected by the statistical analysis and modeling performed (since this is a semester one course on Data Acquisition and Management). 

# Every student must be prepared to explain how the data was collected, loaded, transformed, tidied, and analyzed for outliers, etc. in our Meetup.  This is the only way I’ll have to determine that everyone actively participated in the process, so you need to hold yourself responsible for understanding what your class-size team did!  If you are unable to attend the meet up, then you need to either present to me one-on-one before the meetup presentation, or post a 3 to 5 minute video (e.g. on YouTube) explaining the process.  Individual students will not be responsible for explaining any forays into statistical analysis, modeling, data mining, regression, decision trees, etc. 



# Introduction: This is a group project of team A1. We are four members Mohammad Zahid Chowdhury, Stefan Huber, Tyler Graham and Daniel DeBonis in this group. 


# Effective Collaboration tools: In the data science field, for communication and documentation purposes we have maintained the following tools. 

# Communication:

      ##  Discord and Zoom


# Code Sharing:

      ##  Github


# Project Documentation: 


    ##  Draw.io – ER diagram 
	  ##  RStudio – writing code or data processing 
    ##  Word document- Documenting work done
    ##  Power Point - For Presentation the group project


# Data Sources and Management:
	
# Data is collected from https://www.kaggle.com/datasets/misganawtboltana/data-science-job-market-in-2025-15k. And save this data as CSV file and uploaded it in GitHub.File is available in github link.  


# Required Packages installation and loading:

```{r}

install.packages("tidyverse")
library(tidyverse)
library(dplyr)

install.packages("DBI")
install.packages("RSQLite")

# Load the packages
library(DBI)
library(RSQLite)

```


# Data Loading and reading:


```{r}

project3_data<-read.csv("https://raw.githubusercontent.com/zahid607/Project-3/refs/heads/main/Data%20Science_Jobs%20.csv")

dim(project3_data)
head(project3_data)


```

# View the column names of the dataset


```{r}

colnames(project3_data)

```

# Data Cleaning and transformations:


# Check for missing values in the dataset:

```{r}
sum(is.na(project3_data))

```

# There is a one missing value in the data set.

```{r}
colSums(is.na(project3_data))
```

# There is one company name miising. 


# Find the rows that contain missing values:

```{r}
which(is.na(project3_data), arr.ind = TRUE)
```

# So, the missing value is located at 40th row and 2nd column.


# Check if there are any duplicate rows in the dataset

```{r}
# Check for duplicate rows based on all columns
duplicates <- duplicated(project3_data)

# Count the number of duplicates
num_duplicates <- sum(duplicates)

print(paste("Number of duplicate rows:", num_duplicates))

```

 
```{r}
# Mark duplicates in the data
project3_data$is_duplicate <- duplicated(project3_data) | duplicated(project3_data, fromLast = TRUE)

# Extract rows with duplicates (both the original and the duplicate)
duplicate_rows_with_originals <- project3_data[project3_data$is_duplicate == TRUE, ]

# Print the duplicate rows alongside their originals
print(duplicate_rows_with_originals)

```

 
# Remove duplicates from the dataset (keeps the first occurrence)


```{r}
project3_data_cleaned <- project3_data[!duplicated(project3_data), ]

```


# Dimension of new data set.

```{r}

dim(project3_data_cleaned)

```
# Analysis: Frequency distribution


```{r}

# Separate the skills by comma (if skills are listed in one cell, separated by commas)
project3_data_expanded <- project3_data %>%
  separate_rows(Skills, sep = ",") %>%
  mutate(Skills = trimws(Skills))  # Trim any extra spaces around skills

# Count the frequency of each skill
skill_frequency <- project3_data_expanded %>%
  group_by(Skills) %>%
  tally(sort = TRUE)  # Count and sort in descending order

# Print the skill frequency
print(skill_frequency)

```

# Visualization:

```{r}
# Select top 10 most frequent skills
top_10_skills <- skill_frequency %>%
  top_n(10, n)  # Get the top 10 most frequent skills

# Create the bar chart for the top 10 skills
ggplot(top_10_skills, aes(x = reorder(Skills, n), y = n)) +
  geom_bar(stat = "identity", fill = "skyblue") +  # Create bars
  geom_text(aes(label = n), vjust = -0.3, color = "black") +  # Add frequency annotations on bars
  labs(title = "Top 10 Most Frequent Skills", x = "Skills", y = "Frequency") +  # Labels
  theme_minimal() +  # Clean theme
  coord_flip()  # Flip the chart for better readability

```


```{r}
# Log-transform the frequency counts
skill_frequency <- skill_frequency %>%
  mutate(log_frequency = log(n + 1))  # Use log(n+1) to avoid log(0) for skills with zero occurrences

# Create the log-transformed histogram
ggplot(skill_frequency, aes(x = log_frequency)) +
  geom_histogram(binwidth = 0.2, fill = "skyblue", color = "black") +
  labs(title = "Log-Transformed Histogram of Skill Frequency Distribution", 
       x = "Log of Frequency", 
       y = "Count of Skills") +
  theme_minimal()

```




```{r}
# Create or connect to SQLite database
db <- dbConnect(RSQLite::SQLite(), dbname = "project3_data.db")

# Write the data to the SQLite database
dbWriteTable(db, "project3_data_expanded", project3_data_expanded, overwrite = TRUE)

# List all tables to confirm the data was written
tables <- dbListTables(db)
print(tables)

# Query the top 5 most frequent job titles
query <- "
SELECT Job.Title, COUNT(*) AS Frequency
FROM project3_data_expanded
GROUP BY Job.Title
ORDER BY Frequency DESC
LIMIT 5
"
result <- dbGetQuery(db, query)
print(result)

# Disconnect from the database
dbDisconnect(db)
```



